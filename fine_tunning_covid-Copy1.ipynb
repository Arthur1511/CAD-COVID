{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "lIefVYJR_wfJ",
    "outputId": "568a0b16-7e96-4400-c3ec-94ef348b1327"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.5.0\n",
      "Torchvision Version:  0.6.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "from __future__ import division\n",
    "import itertools\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchxrayvision as xrv\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.exposure import equalize_adapthist\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import sys\n",
    "import dataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q1zxbdpcAVDF"
   },
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms \n",
    "#   to the ImageFolder structure\n",
    "data_dir = \"/home/users/ester/datasets/covidx\"\n",
    "\n",
    "path_train = '/home/arthur/CADCOVID/covidxxx/images/train'\n",
    "path_val = '/home/arthur/CADCOVID/covidxxx/images/test'\n",
    "\n",
    "list_img_train, lbl_train = dataset.get_dataset_info(path_train)\n",
    "list_img_val, lbl_val = dataset.get_dataset_info(path_val)\n",
    "\n",
    "# creating tensorboar file\n",
    "# current_time = datetime.datetime.now().strftime(\"%d%m%Y-%H%M%S\")\n",
    "\n",
    "# writer = SummaryWriter('runs/fine_tunning_covidNIH/' + current_time)\n",
    "\n",
    "# selecting gpu device\n",
    "device = torch.device(\"cuda:2\") # if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "model_name= \"\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 3\n",
    "\n",
    "class_labels = ['pneumonia', 'covid', 'normal']\n",
    "\n",
    "# classes weights\n",
    "weights = [13906/476, 13906/7966, 13906/5464]\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 30\n",
    "\n",
    "# Number of epochs to train for \n",
    "num_epochs = 60\n",
    "\n",
    "lr=0.001\n",
    "# Flag for feature extracting. When False, we finetune the whole model, \n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l2aCsfdFAX3K"
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "    \n",
    "    val_acc_history = []\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            labels_list = []\n",
    "            pred_list = []\n",
    "            \n",
    "            \n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "            \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    \n",
    "                   \n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "#                         optimizer.step()\n",
    "                        scheduler.step()\n",
    "   \n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                labels_list.extend(labels.cpu().numpy())\n",
    "                pred_list.extend(preds.cpu().numpy())\n",
    "\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            writer.add_scalar('Loss/{}'.format(phase) , epoch_loss, epoch)\n",
    "            writer.add_scalar('Accuracy/{}'.format(phase) , epoch_acc, epoch)\n",
    "            \n",
    "            \n",
    "            labels = labels_list\n",
    "            preds = pred_list\n",
    "\n",
    "            balanced_accuracy = balanced_accuracy_score(y_true=labels, y_pred=preds)\n",
    "            precision = precision_score(labels, preds, average=None)\n",
    "            recall = recall_score(labels, preds, average=None)\n",
    "            f1 = f1_score(labels, preds, average=None)\n",
    "\n",
    "            print('{} Balanced Accuracy: {} Precision: {} Recall: {} F1-Score: {}'.format(phase, balanced_accuracy, precision, recall, f1))\n",
    "\n",
    "            writer.add_scalar('Precision pneumonia/{}'.format(phase) , precision[0], epoch)\n",
    "            writer.add_scalar('Recall pneumonia/{}'.format(phase) , recall[0], epoch)\n",
    "            writer.add_scalar('F1 Score pneumonia/{}'.format(phase) , f1[0], epoch)\n",
    "            \n",
    "            writer.add_scalar('Balanced_Accuracy/{}'.format(phase) , balanced_accuracy, epoch)\n",
    "            writer.add_scalar('Precision covid/{}'.format(phase) , precision[1], epoch)\n",
    "            writer.add_scalar('Recall covid/{}'.format(phase) , recall[1], epoch)\n",
    "            writer.add_scalar('F1 Score covid/{}'.format(phase) , f1[1], epoch)\n",
    "\n",
    "            writer.add_scalar('Precision normal/{}'.format(phase) , precision[2], epoch)\n",
    "            writer.add_scalar('Recall normal/{}'.format(phase) , recall[2], epoch)\n",
    "            writer.add_scalar('F1 Score normal/{}'.format(phase) , f1[2], epoch)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "#             print(classification_report(labels, preds, target_names=class_labels))\n",
    "\n",
    "            cm = confusion_matrix(labels, preds)\n",
    "\n",
    "            cm_fig = plot_confusion_matrix(cm, classes=class_labels, normalize=True, title='{} Confusion Matrix'.format(phase))\n",
    "\n",
    "            writer.add_figure('Confusion Matrix Normalized/'+ phase, cm_fig, epoch)\n",
    "            \n",
    "            cm_fig = plot_confusion_matrix(cm, classes=class_labels, normalize=False, title='{} Confusion Matrix'.format(phase))\n",
    "\n",
    "            writer.add_figure('Confusion Matrix/'+ phase, cm_fig, epoch)\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'test' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'test':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    writer.close()\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BhkW97JtAcKd"
   },
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues,\n",
    "                          save=False):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    \n",
    "    fig = plt.figure(figsize=(7, 7))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, '%.2f' % (cm[i, j] * 100.0),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "#     plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ELERlI6F-c8"
   },
   "source": [
    "# Initialize and Reshape the Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "RH8e7QF1ArQa",
    "outputId": "1e8299bd-3219-4260-b69f-607badcb9c8c"
   },
   "outputs": [],
   "source": [
    "model_ft = xrv.models.DenseNet(weights=\"nih\")\n",
    "set_parameter_requires_grad(model_ft, feature_extract)\n",
    "model_ft.classifier = nn.Linear(1024, num_classes)\n",
    "model_ft.pathologies = ['normal', \"pneumonia\", 'COVID-19']\n",
    "model_ft.op_threshs = None\n",
    "input_size = 224\n",
    "# Print the model we just instantiated\n",
    "# print(model_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "diPb9wuPHpIS"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qBe5x_QPGNCq",
    "outputId": "22e98de2-a227-45fe-93b5-55db463c742c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "\n",
    "std = 0.24671278988052675\n",
    "mean = 0.4912771402827791\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "#         transforms.RandomResizedCrop(input_size),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([mean], [std]),\n",
    "          \n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "#         transforms.Resize(input_size),\n",
    "#         transforms.CenterCrop(input_size),\n",
    "#         transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([mean], [std]),\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    ]),\n",
    "}\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# # # Create training and validation datasets\n",
    "# image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'test']}\n",
    "# # Create training and validation dataloaders\n",
    "# dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'test']}\n",
    "\n",
    "\n",
    "tmp_dataset_train = dataset.COVID19_Dataset(list_img_train, lbl_train, transform = data_transforms['train'])\n",
    "tmp_dataset_val = dataset.COVID19_Dataset(list_img_val, lbl_val, transform = data_transforms['test'])\n",
    "\n",
    "# Create a sampler by samples weights \n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(\n",
    "    weights=tmp_dataset_train.samples_weights,\n",
    "    num_samples=tmp_dataset_train.len)\n",
    "\n",
    "\n",
    "dataloaders_dict = {}\n",
    "\n",
    "dataloaders_dict['train'] = torch.utils.data.DataLoader(tmp_dataset_train, \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    sampler=sampler,\n",
    "                                                    num_workers=4)\n",
    "\n",
    "dataloaders_dict['test'] = torch.utils.data.DataLoader(tmp_dataset_val, \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pneumonia', 'covid', 'normal']\n",
      "[0.00018947 0.00211864 0.00014217]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'COVID19_Dataset' object has no attribute 'class_to_idx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c87e22b5f751>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlbl_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtmp_dataset_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_to_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'COVID19_Dataset' object has no attribute 'class_to_idx'"
     ]
    }
   ],
   "source": [
    "print(tmp_dataset_train.classes)\n",
    "\n",
    "print(1. / np.unique(np.array(lbl_train), return_counts=True)[1])\n",
    "tmp_dataset_train.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "-9JI91I27JOr",
    "outputId": "73c122c5-cd04-4366-fb16-ffb79df03bb7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t features.conv0.weight\n",
      "\t features.norm0.weight\n",
      "\t features.norm0.bias\n",
      "\t features.denseblock1.denselayer1.norm1.weight\n",
      "\t features.denseblock1.denselayer1.norm1.bias\n",
      "\t features.denseblock1.denselayer1.conv1.weight\n",
      "\t features.denseblock1.denselayer1.norm2.weight\n",
      "\t features.denseblock1.denselayer1.norm2.bias\n",
      "\t features.denseblock1.denselayer1.conv2.weight\n",
      "\t features.denseblock1.denselayer2.norm1.weight\n",
      "\t features.denseblock1.denselayer2.norm1.bias\n",
      "\t features.denseblock1.denselayer2.conv1.weight\n",
      "\t features.denseblock1.denselayer2.norm2.weight\n",
      "\t features.denseblock1.denselayer2.norm2.bias\n",
      "\t features.denseblock1.denselayer2.conv2.weight\n",
      "\t features.denseblock1.denselayer3.norm1.weight\n",
      "\t features.denseblock1.denselayer3.norm1.bias\n",
      "\t features.denseblock1.denselayer3.conv1.weight\n",
      "\t features.denseblock1.denselayer3.norm2.weight\n",
      "\t features.denseblock1.denselayer3.norm2.bias\n",
      "\t features.denseblock1.denselayer3.conv2.weight\n",
      "\t features.denseblock1.denselayer4.norm1.weight\n",
      "\t features.denseblock1.denselayer4.norm1.bias\n",
      "\t features.denseblock1.denselayer4.conv1.weight\n",
      "\t features.denseblock1.denselayer4.norm2.weight\n",
      "\t features.denseblock1.denselayer4.norm2.bias\n",
      "\t features.denseblock1.denselayer4.conv2.weight\n",
      "\t features.denseblock1.denselayer5.norm1.weight\n",
      "\t features.denseblock1.denselayer5.norm1.bias\n",
      "\t features.denseblock1.denselayer5.conv1.weight\n",
      "\t features.denseblock1.denselayer5.norm2.weight\n",
      "\t features.denseblock1.denselayer5.norm2.bias\n",
      "\t features.denseblock1.denselayer5.conv2.weight\n",
      "\t features.denseblock1.denselayer6.norm1.weight\n",
      "\t features.denseblock1.denselayer6.norm1.bias\n",
      "\t features.denseblock1.denselayer6.conv1.weight\n",
      "\t features.denseblock1.denselayer6.norm2.weight\n",
      "\t features.denseblock1.denselayer6.norm2.bias\n",
      "\t features.denseblock1.denselayer6.conv2.weight\n",
      "\t features.transition1.norm.weight\n",
      "\t features.transition1.norm.bias\n",
      "\t features.transition1.conv.weight\n",
      "\t features.denseblock2.denselayer1.norm1.weight\n",
      "\t features.denseblock2.denselayer1.norm1.bias\n",
      "\t features.denseblock2.denselayer1.conv1.weight\n",
      "\t features.denseblock2.denselayer1.norm2.weight\n",
      "\t features.denseblock2.denselayer1.norm2.bias\n",
      "\t features.denseblock2.denselayer1.conv2.weight\n",
      "\t features.denseblock2.denselayer2.norm1.weight\n",
      "\t features.denseblock2.denselayer2.norm1.bias\n",
      "\t features.denseblock2.denselayer2.conv1.weight\n",
      "\t features.denseblock2.denselayer2.norm2.weight\n",
      "\t features.denseblock2.denselayer2.norm2.bias\n",
      "\t features.denseblock2.denselayer2.conv2.weight\n",
      "\t features.denseblock2.denselayer3.norm1.weight\n",
      "\t features.denseblock2.denselayer3.norm1.bias\n",
      "\t features.denseblock2.denselayer3.conv1.weight\n",
      "\t features.denseblock2.denselayer3.norm2.weight\n",
      "\t features.denseblock2.denselayer3.norm2.bias\n",
      "\t features.denseblock2.denselayer3.conv2.weight\n",
      "\t features.denseblock2.denselayer4.norm1.weight\n",
      "\t features.denseblock2.denselayer4.norm1.bias\n",
      "\t features.denseblock2.denselayer4.conv1.weight\n",
      "\t features.denseblock2.denselayer4.norm2.weight\n",
      "\t features.denseblock2.denselayer4.norm2.bias\n",
      "\t features.denseblock2.denselayer4.conv2.weight\n",
      "\t features.denseblock2.denselayer5.norm1.weight\n",
      "\t features.denseblock2.denselayer5.norm1.bias\n",
      "\t features.denseblock2.denselayer5.conv1.weight\n",
      "\t features.denseblock2.denselayer5.norm2.weight\n",
      "\t features.denseblock2.denselayer5.norm2.bias\n",
      "\t features.denseblock2.denselayer5.conv2.weight\n",
      "\t features.denseblock2.denselayer6.norm1.weight\n",
      "\t features.denseblock2.denselayer6.norm1.bias\n",
      "\t features.denseblock2.denselayer6.conv1.weight\n",
      "\t features.denseblock2.denselayer6.norm2.weight\n",
      "\t features.denseblock2.denselayer6.norm2.bias\n",
      "\t features.denseblock2.denselayer6.conv2.weight\n",
      "\t features.denseblock2.denselayer7.norm1.weight\n",
      "\t features.denseblock2.denselayer7.norm1.bias\n",
      "\t features.denseblock2.denselayer7.conv1.weight\n",
      "\t features.denseblock2.denselayer7.norm2.weight\n",
      "\t features.denseblock2.denselayer7.norm2.bias\n",
      "\t features.denseblock2.denselayer7.conv2.weight\n",
      "\t features.denseblock2.denselayer8.norm1.weight\n",
      "\t features.denseblock2.denselayer8.norm1.bias\n",
      "\t features.denseblock2.denselayer8.conv1.weight\n",
      "\t features.denseblock2.denselayer8.norm2.weight\n",
      "\t features.denseblock2.denselayer8.norm2.bias\n",
      "\t features.denseblock2.denselayer8.conv2.weight\n",
      "\t features.denseblock2.denselayer9.norm1.weight\n",
      "\t features.denseblock2.denselayer9.norm1.bias\n",
      "\t features.denseblock2.denselayer9.conv1.weight\n",
      "\t features.denseblock2.denselayer9.norm2.weight\n",
      "\t features.denseblock2.denselayer9.norm2.bias\n",
      "\t features.denseblock2.denselayer9.conv2.weight\n",
      "\t features.denseblock2.denselayer10.norm1.weight\n",
      "\t features.denseblock2.denselayer10.norm1.bias\n",
      "\t features.denseblock2.denselayer10.conv1.weight\n",
      "\t features.denseblock2.denselayer10.norm2.weight\n",
      "\t features.denseblock2.denselayer10.norm2.bias\n",
      "\t features.denseblock2.denselayer10.conv2.weight\n",
      "\t features.denseblock2.denselayer11.norm1.weight\n",
      "\t features.denseblock2.denselayer11.norm1.bias\n",
      "\t features.denseblock2.denselayer11.conv1.weight\n",
      "\t features.denseblock2.denselayer11.norm2.weight\n",
      "\t features.denseblock2.denselayer11.norm2.bias\n",
      "\t features.denseblock2.denselayer11.conv2.weight\n",
      "\t features.denseblock2.denselayer12.norm1.weight\n",
      "\t features.denseblock2.denselayer12.norm1.bias\n",
      "\t features.denseblock2.denselayer12.conv1.weight\n",
      "\t features.denseblock2.denselayer12.norm2.weight\n",
      "\t features.denseblock2.denselayer12.norm2.bias\n",
      "\t features.denseblock2.denselayer12.conv2.weight\n",
      "\t features.transition2.norm.weight\n",
      "\t features.transition2.norm.bias\n",
      "\t features.transition2.conv.weight\n",
      "\t features.denseblock3.denselayer1.norm1.weight\n",
      "\t features.denseblock3.denselayer1.norm1.bias\n",
      "\t features.denseblock3.denselayer1.conv1.weight\n",
      "\t features.denseblock3.denselayer1.norm2.weight\n",
      "\t features.denseblock3.denselayer1.norm2.bias\n",
      "\t features.denseblock3.denselayer1.conv2.weight\n",
      "\t features.denseblock3.denselayer2.norm1.weight\n",
      "\t features.denseblock3.denselayer2.norm1.bias\n",
      "\t features.denseblock3.denselayer2.conv1.weight\n",
      "\t features.denseblock3.denselayer2.norm2.weight\n",
      "\t features.denseblock3.denselayer2.norm2.bias\n",
      "\t features.denseblock3.denselayer2.conv2.weight\n",
      "\t features.denseblock3.denselayer3.norm1.weight\n",
      "\t features.denseblock3.denselayer3.norm1.bias\n",
      "\t features.denseblock3.denselayer3.conv1.weight\n",
      "\t features.denseblock3.denselayer3.norm2.weight\n",
      "\t features.denseblock3.denselayer3.norm2.bias\n",
      "\t features.denseblock3.denselayer3.conv2.weight\n",
      "\t features.denseblock3.denselayer4.norm1.weight\n",
      "\t features.denseblock3.denselayer4.norm1.bias\n",
      "\t features.denseblock3.denselayer4.conv1.weight\n",
      "\t features.denseblock3.denselayer4.norm2.weight\n",
      "\t features.denseblock3.denselayer4.norm2.bias\n",
      "\t features.denseblock3.denselayer4.conv2.weight\n",
      "\t features.denseblock3.denselayer5.norm1.weight\n",
      "\t features.denseblock3.denselayer5.norm1.bias\n",
      "\t features.denseblock3.denselayer5.conv1.weight\n",
      "\t features.denseblock3.denselayer5.norm2.weight\n",
      "\t features.denseblock3.denselayer5.norm2.bias\n",
      "\t features.denseblock3.denselayer5.conv2.weight\n",
      "\t features.denseblock3.denselayer6.norm1.weight\n",
      "\t features.denseblock3.denselayer6.norm1.bias\n",
      "\t features.denseblock3.denselayer6.conv1.weight\n",
      "\t features.denseblock3.denselayer6.norm2.weight\n",
      "\t features.denseblock3.denselayer6.norm2.bias\n",
      "\t features.denseblock3.denselayer6.conv2.weight\n",
      "\t features.denseblock3.denselayer7.norm1.weight\n",
      "\t features.denseblock3.denselayer7.norm1.bias\n",
      "\t features.denseblock3.denselayer7.conv1.weight\n",
      "\t features.denseblock3.denselayer7.norm2.weight\n",
      "\t features.denseblock3.denselayer7.norm2.bias\n",
      "\t features.denseblock3.denselayer7.conv2.weight\n",
      "\t features.denseblock3.denselayer8.norm1.weight\n",
      "\t features.denseblock3.denselayer8.norm1.bias\n",
      "\t features.denseblock3.denselayer8.conv1.weight\n",
      "\t features.denseblock3.denselayer8.norm2.weight\n",
      "\t features.denseblock3.denselayer8.norm2.bias\n",
      "\t features.denseblock3.denselayer8.conv2.weight\n",
      "\t features.denseblock3.denselayer9.norm1.weight\n",
      "\t features.denseblock3.denselayer9.norm1.bias\n",
      "\t features.denseblock3.denselayer9.conv1.weight\n",
      "\t features.denseblock3.denselayer9.norm2.weight\n",
      "\t features.denseblock3.denselayer9.norm2.bias\n",
      "\t features.denseblock3.denselayer9.conv2.weight\n",
      "\t features.denseblock3.denselayer10.norm1.weight\n",
      "\t features.denseblock3.denselayer10.norm1.bias\n",
      "\t features.denseblock3.denselayer10.conv1.weight\n",
      "\t features.denseblock3.denselayer10.norm2.weight\n",
      "\t features.denseblock3.denselayer10.norm2.bias\n",
      "\t features.denseblock3.denselayer10.conv2.weight\n",
      "\t features.denseblock3.denselayer11.norm1.weight\n",
      "\t features.denseblock3.denselayer11.norm1.bias\n",
      "\t features.denseblock3.denselayer11.conv1.weight\n",
      "\t features.denseblock3.denselayer11.norm2.weight\n",
      "\t features.denseblock3.denselayer11.norm2.bias\n",
      "\t features.denseblock3.denselayer11.conv2.weight\n",
      "\t features.denseblock3.denselayer12.norm1.weight\n",
      "\t features.denseblock3.denselayer12.norm1.bias\n",
      "\t features.denseblock3.denselayer12.conv1.weight\n",
      "\t features.denseblock3.denselayer12.norm2.weight\n",
      "\t features.denseblock3.denselayer12.norm2.bias\n",
      "\t features.denseblock3.denselayer12.conv2.weight\n",
      "\t features.denseblock3.denselayer13.norm1.weight\n",
      "\t features.denseblock3.denselayer13.norm1.bias\n",
      "\t features.denseblock3.denselayer13.conv1.weight\n",
      "\t features.denseblock3.denselayer13.norm2.weight\n",
      "\t features.denseblock3.denselayer13.norm2.bias\n",
      "\t features.denseblock3.denselayer13.conv2.weight\n",
      "\t features.denseblock3.denselayer14.norm1.weight\n",
      "\t features.denseblock3.denselayer14.norm1.bias\n",
      "\t features.denseblock3.denselayer14.conv1.weight\n",
      "\t features.denseblock3.denselayer14.norm2.weight\n",
      "\t features.denseblock3.denselayer14.norm2.bias\n",
      "\t features.denseblock3.denselayer14.conv2.weight\n",
      "\t features.denseblock3.denselayer15.norm1.weight\n",
      "\t features.denseblock3.denselayer15.norm1.bias\n",
      "\t features.denseblock3.denselayer15.conv1.weight\n",
      "\t features.denseblock3.denselayer15.norm2.weight\n",
      "\t features.denseblock3.denselayer15.norm2.bias\n",
      "\t features.denseblock3.denselayer15.conv2.weight\n",
      "\t features.denseblock3.denselayer16.norm1.weight\n",
      "\t features.denseblock3.denselayer16.norm1.bias\n",
      "\t features.denseblock3.denselayer16.conv1.weight\n",
      "\t features.denseblock3.denselayer16.norm2.weight\n",
      "\t features.denseblock3.denselayer16.norm2.bias\n",
      "\t features.denseblock3.denselayer16.conv2.weight\n",
      "\t features.denseblock3.denselayer17.norm1.weight\n",
      "\t features.denseblock3.denselayer17.norm1.bias\n",
      "\t features.denseblock3.denselayer17.conv1.weight\n",
      "\t features.denseblock3.denselayer17.norm2.weight\n",
      "\t features.denseblock3.denselayer17.norm2.bias\n",
      "\t features.denseblock3.denselayer17.conv2.weight\n",
      "\t features.denseblock3.denselayer18.norm1.weight\n",
      "\t features.denseblock3.denselayer18.norm1.bias\n",
      "\t features.denseblock3.denselayer18.conv1.weight\n",
      "\t features.denseblock3.denselayer18.norm2.weight\n",
      "\t features.denseblock3.denselayer18.norm2.bias\n",
      "\t features.denseblock3.denselayer18.conv2.weight\n",
      "\t features.denseblock3.denselayer19.norm1.weight\n",
      "\t features.denseblock3.denselayer19.norm1.bias\n",
      "\t features.denseblock3.denselayer19.conv1.weight\n",
      "\t features.denseblock3.denselayer19.norm2.weight\n",
      "\t features.denseblock3.denselayer19.norm2.bias\n",
      "\t features.denseblock3.denselayer19.conv2.weight\n",
      "\t features.denseblock3.denselayer20.norm1.weight\n",
      "\t features.denseblock3.denselayer20.norm1.bias\n",
      "\t features.denseblock3.denselayer20.conv1.weight\n",
      "\t features.denseblock3.denselayer20.norm2.weight\n",
      "\t features.denseblock3.denselayer20.norm2.bias\n",
      "\t features.denseblock3.denselayer20.conv2.weight\n",
      "\t features.denseblock3.denselayer21.norm1.weight\n",
      "\t features.denseblock3.denselayer21.norm1.bias\n",
      "\t features.denseblock3.denselayer21.conv1.weight\n",
      "\t features.denseblock3.denselayer21.norm2.weight\n",
      "\t features.denseblock3.denselayer21.norm2.bias\n",
      "\t features.denseblock3.denselayer21.conv2.weight\n",
      "\t features.denseblock3.denselayer22.norm1.weight\n",
      "\t features.denseblock3.denselayer22.norm1.bias\n",
      "\t features.denseblock3.denselayer22.conv1.weight\n",
      "\t features.denseblock3.denselayer22.norm2.weight\n",
      "\t features.denseblock3.denselayer22.norm2.bias\n",
      "\t features.denseblock3.denselayer22.conv2.weight\n",
      "\t features.denseblock3.denselayer23.norm1.weight\n",
      "\t features.denseblock3.denselayer23.norm1.bias\n",
      "\t features.denseblock3.denselayer23.conv1.weight\n",
      "\t features.denseblock3.denselayer23.norm2.weight\n",
      "\t features.denseblock3.denselayer23.norm2.bias\n",
      "\t features.denseblock3.denselayer23.conv2.weight\n",
      "\t features.denseblock3.denselayer24.norm1.weight\n",
      "\t features.denseblock3.denselayer24.norm1.bias\n",
      "\t features.denseblock3.denselayer24.conv1.weight\n",
      "\t features.denseblock3.denselayer24.norm2.weight\n",
      "\t features.denseblock3.denselayer24.norm2.bias\n",
      "\t features.denseblock3.denselayer24.conv2.weight\n",
      "\t features.transition3.norm.weight\n",
      "\t features.transition3.norm.bias\n",
      "\t features.transition3.conv.weight\n",
      "\t features.denseblock4.denselayer1.norm1.weight\n",
      "\t features.denseblock4.denselayer1.norm1.bias\n",
      "\t features.denseblock4.denselayer1.conv1.weight\n",
      "\t features.denseblock4.denselayer1.norm2.weight\n",
      "\t features.denseblock4.denselayer1.norm2.bias\n",
      "\t features.denseblock4.denselayer1.conv2.weight\n",
      "\t features.denseblock4.denselayer2.norm1.weight\n",
      "\t features.denseblock4.denselayer2.norm1.bias\n",
      "\t features.denseblock4.denselayer2.conv1.weight\n",
      "\t features.denseblock4.denselayer2.norm2.weight\n",
      "\t features.denseblock4.denselayer2.norm2.bias\n",
      "\t features.denseblock4.denselayer2.conv2.weight\n",
      "\t features.denseblock4.denselayer3.norm1.weight\n",
      "\t features.denseblock4.denselayer3.norm1.bias\n",
      "\t features.denseblock4.denselayer3.conv1.weight\n",
      "\t features.denseblock4.denselayer3.norm2.weight\n",
      "\t features.denseblock4.denselayer3.norm2.bias\n",
      "\t features.denseblock4.denselayer3.conv2.weight\n",
      "\t features.denseblock4.denselayer4.norm1.weight\n",
      "\t features.denseblock4.denselayer4.norm1.bias\n",
      "\t features.denseblock4.denselayer4.conv1.weight\n",
      "\t features.denseblock4.denselayer4.norm2.weight\n",
      "\t features.denseblock4.denselayer4.norm2.bias\n",
      "\t features.denseblock4.denselayer4.conv2.weight\n",
      "\t features.denseblock4.denselayer5.norm1.weight\n",
      "\t features.denseblock4.denselayer5.norm1.bias\n",
      "\t features.denseblock4.denselayer5.conv1.weight\n",
      "\t features.denseblock4.denselayer5.norm2.weight\n",
      "\t features.denseblock4.denselayer5.norm2.bias\n",
      "\t features.denseblock4.denselayer5.conv2.weight\n",
      "\t features.denseblock4.denselayer6.norm1.weight\n",
      "\t features.denseblock4.denselayer6.norm1.bias\n",
      "\t features.denseblock4.denselayer6.conv1.weight\n",
      "\t features.denseblock4.denselayer6.norm2.weight\n",
      "\t features.denseblock4.denselayer6.norm2.bias\n",
      "\t features.denseblock4.denselayer6.conv2.weight\n",
      "\t features.denseblock4.denselayer7.norm1.weight\n",
      "\t features.denseblock4.denselayer7.norm1.bias\n",
      "\t features.denseblock4.denselayer7.conv1.weight\n",
      "\t features.denseblock4.denselayer7.norm2.weight\n",
      "\t features.denseblock4.denselayer7.norm2.bias\n",
      "\t features.denseblock4.denselayer7.conv2.weight\n",
      "\t features.denseblock4.denselayer8.norm1.weight\n",
      "\t features.denseblock4.denselayer8.norm1.bias\n",
      "\t features.denseblock4.denselayer8.conv1.weight\n",
      "\t features.denseblock4.denselayer8.norm2.weight\n",
      "\t features.denseblock4.denselayer8.norm2.bias\n",
      "\t features.denseblock4.denselayer8.conv2.weight\n",
      "\t features.denseblock4.denselayer9.norm1.weight\n",
      "\t features.denseblock4.denselayer9.norm1.bias\n",
      "\t features.denseblock4.denselayer9.conv1.weight\n",
      "\t features.denseblock4.denselayer9.norm2.weight\n",
      "\t features.denseblock4.denselayer9.norm2.bias\n",
      "\t features.denseblock4.denselayer9.conv2.weight\n",
      "\t features.denseblock4.denselayer10.norm1.weight\n",
      "\t features.denseblock4.denselayer10.norm1.bias\n",
      "\t features.denseblock4.denselayer10.conv1.weight\n",
      "\t features.denseblock4.denselayer10.norm2.weight\n",
      "\t features.denseblock4.denselayer10.norm2.bias\n",
      "\t features.denseblock4.denselayer10.conv2.weight\n",
      "\t features.denseblock4.denselayer11.norm1.weight\n",
      "\t features.denseblock4.denselayer11.norm1.bias\n",
      "\t features.denseblock4.denselayer11.conv1.weight\n",
      "\t features.denseblock4.denselayer11.norm2.weight\n",
      "\t features.denseblock4.denselayer11.norm2.bias\n",
      "\t features.denseblock4.denselayer11.conv2.weight\n",
      "\t features.denseblock4.denselayer12.norm1.weight\n",
      "\t features.denseblock4.denselayer12.norm1.bias\n",
      "\t features.denseblock4.denselayer12.conv1.weight\n",
      "\t features.denseblock4.denselayer12.norm2.weight\n",
      "\t features.denseblock4.denselayer12.norm2.bias\n",
      "\t features.denseblock4.denselayer12.conv2.weight\n",
      "\t features.denseblock4.denselayer13.norm1.weight\n",
      "\t features.denseblock4.denselayer13.norm1.bias\n",
      "\t features.denseblock4.denselayer13.conv1.weight\n",
      "\t features.denseblock4.denselayer13.norm2.weight\n",
      "\t features.denseblock4.denselayer13.norm2.bias\n",
      "\t features.denseblock4.denselayer13.conv2.weight\n",
      "\t features.denseblock4.denselayer14.norm1.weight\n",
      "\t features.denseblock4.denselayer14.norm1.bias\n",
      "\t features.denseblock4.denselayer14.conv1.weight\n",
      "\t features.denseblock4.denselayer14.norm2.weight\n",
      "\t features.denseblock4.denselayer14.norm2.bias\n",
      "\t features.denseblock4.denselayer14.conv2.weight\n",
      "\t features.denseblock4.denselayer15.norm1.weight\n",
      "\t features.denseblock4.denselayer15.norm1.bias\n",
      "\t features.denseblock4.denselayer15.conv1.weight\n",
      "\t features.denseblock4.denselayer15.norm2.weight\n",
      "\t features.denseblock4.denselayer15.norm2.bias\n",
      "\t features.denseblock4.denselayer15.conv2.weight\n",
      "\t features.denseblock4.denselayer16.norm1.weight\n",
      "\t features.denseblock4.denselayer16.norm1.bias\n",
      "\t features.denseblock4.denselayer16.conv1.weight\n",
      "\t features.denseblock4.denselayer16.norm2.weight\n",
      "\t features.denseblock4.denselayer16.norm2.bias\n",
      "\t features.denseblock4.denselayer16.conv2.weight\n",
      "\t features.norm5.weight\n",
      "\t features.norm5.bias\n",
      "\t classifier.weight\n",
      "\t classifier.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are \n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(params_to_update, lr=lr, weight_decay=1e-5, amsgrad=True)\n",
    "\n",
    "scheduler = StepLR(optimizer_ft, step_size=20, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir=runs --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "HQI_fRAk7JOv",
    "outputId": "7a492623-d1a4-49b9-ce45-cae667c020e5",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/59\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# weights = torch.Tensor(weights).to(device)\n",
    "\n",
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.Tensor(dataloaders_dict['train'].dataset.weight_class).to(device))\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, scheduler, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "adh0gzugl1nT"
   },
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(), './xrayvision_nih_ft.pt')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "fine_tunning_covid.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
